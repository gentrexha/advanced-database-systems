---
title: "Exercise Sheet 2, 2019"
subtitle: "6.0 VU Advanced Database Systems"
author: "Gent Rexha (11832486), Princ Mullatahiri (11846033)"
date: "08.05.2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exercise 1 (MapReduce)[4 Punkte]

### a

For this, we take the input and map the author to the row in the `checkouts-by-title.csv` file. After that most of the heavy lifting is being done in the reducer, where a hashmap of all the titles of the current creator is created and for each value in our values list the number of checkouts is updated for the corresponding title. After having iterated all the values, the title with the maximum amount of checkouts is written out with it's corresponding author.

### b

## Exercise 2 (Costs of MapReduce)[1 Punkte]

### a

If not using a combiner, there's some skew to be expected in the Reducer considering different keys can receive different amount of values and therefore result into different processing times. For example there can be a lot of values for key_A but very few or none for key_B, resulting in more calculation for the values in key_A and therefore more processing time. 

This is till dependent on the `key,value` data distribution, because in the extreme case where all of your values are of the same key, the number of reducers doesn't matter. In this case a custom partitioner might make more sense.

### b

Considering a small number of reducers, we expect more randomized key values at different reducers and a more even distribution of high processing values of one key. This also results in less parellesim of reducers and might take longer.

If the numbers of reducers is increased, the probability of only one reducer ending up with with a key,value pair where there's a lot of values is also higher, therefore increasing the skewness to a very significant level. On the other hand a maximum of parellelism is achieved, but with this comes the problem of computation overhead.

### c

### d

## Exercise 3 (Relational Operations)[2 Punkte]

![alt text](C:\Princi\TU Wien\Semestri 1\Advanced Database Systems\Exercise\advanced_db\Exercise 2\Submission\images\Exercise3.png)

### a: Bag Union
code:
sum = 0
foreach val:value 
	sum = sum + 1
context.write(Key, sum)

First we get all values from the same key in reducer, then we iterate through each value and increment sum variable, then we simply write to output key and sum as value.

### b
code:
first group by table origin and seperate them in diffrent vales : value 1 and value 2 corresponding to input file 1 and input file 2
sum_value1 = 0
sum_value2 = 0
foreach val: value 1
	sum_value1 = sum_value1 + 1
foreach val: value 2
	sum_value2 = sum_value2 + 1

if sum_value1 >= sum_value2
	context.write(Key, sum_value1 - sum_value2)
	
First we group by table origin, then sum up values into two different variables sum_value1 and sum_value2, then we substract these values if value is positive we write to output else just skip that Key
 

dif = 0
foreach val:value 
	sum = sum + 1
context.write(Key, sum)

## Exercise 4 (Hive Exercise)[4 Punkte]

Placeholder

## Exercise 5 (Spark in Scala)[4 Punkte]

Placeholder

## References
