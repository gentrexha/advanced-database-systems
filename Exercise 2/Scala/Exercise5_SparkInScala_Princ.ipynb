{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 - Spark in Scala _[4 points]_\n",
    "\n",
    "In this exercise you have to solve the tasks given below. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Elementary RDD functions \n",
    "\n",
    "(Brushing up on the basics of functional programming Scala)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  You are given a list of the first 20 numbers of the Fibbonacci numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val fibs20 = sc.parallelize(List( 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Produce a list that contains all even number from the list 'fibs20':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val evens = fibs20.filter(_ % 2 == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Compute the average value of the list 'fibs20':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val avg = fibs20.map(_).sum / fibs20.length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Produce a list of that shows for each element of the list 'fibs20' its absolute difference from the average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ada =  fibs20.map(Math.abs(_ - avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  You are given a random list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val words = sc.parallelize(List(\"automaton\", \"language\", \"logic\",\"closure\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Furthermore, we define a function that maps a word to its list of permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutate (word:String) = word.permutations.toList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Produce a single list containing all permutations of elements from the list 'words':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val prem = words.map(_).permutate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) From SQL to Dataframe (and back again)\n",
    "\n",
    "#### Find for each of the Spark SQL queries an equivalent one that only uses the Dataframe API (or vice versa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dataPath = \"/home/adbs/2019S/shared/diamonds.csv\"\n",
    "val diamonds = spark.read.format(\"csv\")\n",
    "  .option(\"header\",\"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load(dataPath)\n",
    "diamonds.createOrReplaceTempView(\"diamonds\")\n",
    "\n",
    "val articlesDF = spark.read.format(\"json\").load(\"/home/adbs/2019S/shared/spark/nytarticles\")\n",
    "val commentsDF = spark.read.json(\"/home/adbs/2019S/shared/spark/nytcomments\")\n",
    "articlesDF.createOrReplaceTempView(\"articles\")\n",
    "commentsDF.createOrReplaceTempView(\"comments\")\n",
    "// Create RDD view into dataset\n",
    "val articlesRDD = articlesDF.rdd\n",
    "val commentsRDD = commentsDF.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 1: Transform the given Spark SQL query into the Dataframe API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val query1 = spark.sql(\"SELECT COUNT(*) FROM articles WHERE sectionName='Politics'\")\n",
    "query1.show()\n",
    "query1.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val query1_df = articlesDF.filter($\"sectionName\" = 'Politics').count()\n",
    "query1_df.show()\n",
    "query1_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 2: Transform the given Dataframe API query into Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val query2 = articlesDF.groupBy(\"sectionName\").count()\n",
    "query2.show(false)\n",
    "query2.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val query2_sql = spark.sql(\"SELECT Count(*) FROM articles group by sectionName\")\n",
    "query2_sql.show()\n",
    "query2_sql.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 3: Transform the given Spark SQL query into the Dataframe API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val query3  = spark.sql(\n",
    "    \"SELECT a.headline, COUNT(c.commentID) AS numComments FROM articles a, comments c WHERE a.articleID = c.articleID GROUP BY a.headline\" )\n",
    "query3.show(false) // 'false' turns of truncation of row entries\n",
    "query3.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val joinedTables = articlesDF.join(commentsDF, articlesDF(\"articleID\") === commentsDF(\"articleID\"))\n",
    "val query3_df = joinedTables.groupBy(\"headline\").count().as(\"numComments\")\n",
    "query3_df.show()\n",
    "query3.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 4: Transform the given Spark SQL query into the Dataframe API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val query4 = spark.sql(\" SELECT headline, byline, pubDate FROM articles WHERE headline RLIKE \\\"2016\\\" \")\n",
    "query4.show(false)\n",
    "query4.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val query4_df = articlesDF.select($\"headline\", $\"byline\", $\"pubDate\").rlike(\"2016\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 5: Transform the given Dataframe API query into Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val query5 = articlesDF\n",
    "      .join(commentsDF, articlesDF(\"articleID\") === commentsDF(\"articleID\"))\n",
    "      .select(explode(articlesDF(\"keywords\")).as(\"singleKeyWords\"))\n",
    "      .groupBy(\"singleKeyWords\")\n",
    "      .agg(count(\"singleKeyWords\").as(\"number\"))\n",
    "      .orderBy(desc(\"number\"))\n",
    "query5.show(false)\n",
    "query5.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that \"explode\" is a Spark SQL function that turns a tuple with column that contains a collection of objects into multiple tuples each with a single value from this collection. More information here: https://spark.apache.org/docs/2.3.0/api/sql/index.html#explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val query5_sql = spark.sql(\"Select a.keyWords as singleKeyWords, Count(keywords) as numer FROM articles a, comments c where a.articleID = c.articleID group by a.keyWords order by number desc\")\n",
    "query5_sql.show()\n",
    "query5.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For All Queries Above: \n",
    "#### Analyze the plans (.explain() ) and compare performance (using the Spark Web UI). Try to reason about any major differences in the logical plans (if there are any)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Wide and Narrow Dependencies\n",
    "\n",
    "#### Look at the Dataframe queries given as part of b) or for which you wrote the Dataframe version.\n",
    "\n",
    "#### Use the Spark Internal Web UI to analyse the dependencies and stages of the queries, and try to determine which commands on which Dataframes are executed as wide dependencies and which as narrow dependencies. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (spylon-kernel)",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
